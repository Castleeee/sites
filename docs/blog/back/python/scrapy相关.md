---
title: Scrapy(old)
date: 2018-7-13 14:40:45
prev: ./åŸºç¡€çˆ¬è™«.md
next: false
categories: backEnd
tags:
- pythonğŸ
- scrapy
- åç«¯
- å¼‚æ­¥
- æ‰€æœ‰æ–‡ç« 
- çˆ¬è™«ğŸ•·
- è¿›é˜¶
---


<div align= center><h1>Scrapyä»‹ç»</h1></div>
<div align= center><h3>å¼‚æ­¥</h3>
engineå¼‚æ­¥è°ƒç”¨ç¼©çŸ­äº‹ä»¶
<h3>äº‹ä»¶å¾ªç¯</h3>
å¯¹è±¡ä¼ é€’+äº‹ä»¶å¾ªç¯<br/>
spiderâ€”>itemâ€”>pipline
<h3>å®šåˆ¶åŒ–</h3>
å®šåˆ¶çš„piplineé›†ä¸­å­˜å‚¨<br/>
selector+xpathå®šä½çˆ¬å–
<img src="./static/scrapy_architecture.png"/></div>

## Setting&å‘½ä»¤
<h3>ç¯å¢ƒ</h3>

è®¾ç½®å¥½ç¯å¢ƒpythonè·¯å¾„ç­‰ç­‰<br/>
ä»wheelæ‰¾
- pywin32
- pillow
- tiwsted
:::danger
ä¸èƒ½åªè£…pymysqléœ€è¦MYSQLdb<br/>
å®‰è£…MYSQLdbä¹Ÿå°±æ˜¯mydqlclientå’Œpymysql<br/>
å¦‚æœå¤±è´¥apt-get install python-devel mysql-devel
:::

<h3>httpçŠ¶æ€ç </h3>



          codeè¯´æ˜
          200è¯·æ±‚è¢«æˆåŠŸå¤„ç†
          301/302æ°¸ä¹…æ€§é‡å®šå‘/ä¸´æ—¶æ€§é‡å®šå‘
          403æ²¡æœ‰æƒé™è®¿é—®
          404è¡¨ç¤ºæ²¡æœ‰å¯¹åº”çš„èµ„æº
          500æœåŠ¡å™¨é”™è¯¯
          503æœåŠ¡å™¨åœæœºæˆ–æ­£åœ¨ç»´æŠ¤

<h3>å‘½ä»¤è¡Œ</h3>


          scrapy startproject name
          #æ–°å»ºä¸€ä¸ªå·¥ç¨‹,å¹¶cd è¿›ç›®å½•ä¸­å»
          scrapy genspider [-t crawl] name xxx.com
          #ç”Ÿæˆä¸€ä¸ªxxxçˆ¬è™«ï¼Œæ³¨æ„åŸŸååªéœ€è¦www.xxx.comå°±è¡ŒäºŒçº§åŸŸåç›´æ¥å¡«
          ä¸å¡«å†™å°±æ˜¯é»˜è®¤çš„ spider å¡«å†™æœ€å¸¸ç”¨çš„å°±æ˜¯crawl
          scrapy crawl spidername
          #å‘½ä»¤è¡Œè¿è¡Œspidernameçˆ¬è™«


:::warning
ä½ æ–°å»ºçš„çˆ¬è™«åå­—ä¸èƒ½ä¸ç½‘ç«™åå­—ï¼Œé¡¹ç›®åå­—ç›¸åŒï¼Œä½ çš„åå­—ä¼šåœ¨spiderä¸­æœ‰ä¸€ä¸ªå˜é‡è¡¨ç¤º
:::

<h3>pycharmå¯åŠ¨</h3>
æ–°å»ºä¸€ä¸ªpyæ–‡ä»¶å†™å…¥å¦‚ä¸‹å†…å®¹ï¼Œåœ¨pycharmä¸­ä½œå¦‚ä¸‹è®¾ç½®

```py
#import your model here
from scrapy import cmdline
#your class&function here

if __name__ == "__main__":
    cmdline.execute(["scrapy", "crawl", "çˆ¬è™«åå­—"])

```
<div align= center><img src="./static/starter.png"/></div>

è¿™æ ·ç›´æ¥F5å°±å¯ä»¥è¿è¡Œæ•´ä¸ªç¨‹åºäº†
<h3>ä¿®æ”¹setting</h3>

```py
import os,sys
project_dir=os.path.abspath(os.path.dirname(__file__))#è·å–äº†å½“å‰æ–‡ä»¶ä¹Ÿå°±æ˜¯settingçš„ç›®å½•
IMAGES_STORE = os.path.join(project_dir,"images")#è®¾ç½®imageçš„å­˜å‚¨æ–‡ä»¶å¤¹ä¸ºå½“å‰ç›®å½•ä¸‹çš„image
```

:::tip (oï¾Ÿvï¾Ÿ)ãƒ
æŠŠsettingä¸­çš„ROBOTSTXT_OBEY = Trueè®¾ç½®ä¸ºFalseä¸ç„¶ä¼šå¾ˆè ¢
:::
æ›´å¤šè®¾ç½®çš„ä½œç”¨åœ¨<a href='#settingçš„ä¸€äº›å¸¸ç”¨'>è¿™é‡Œ</a>
## Spider
### æ™®é€šspider
æ–°å»ºäº†ä¸€ä¸ªspideræ˜¯è¿™æ ·çš„<br/>
<div align= center ><img style='height:250px' src="./static/basicspider.png"/></div>

è¿™ä¸ªç±»æ¥ç»§æ‰¿äº†Spider
- nameå°±æ˜¯çˆ¬è™«åå­—
- allow_dominså°±æ˜¯å…è®¸çˆ¬å“ªä¸ªåŸŸåçš„èŒƒå›´
- start_urlsæ˜¯ä¸€ä¸ªå¼€å§‹çˆ¬å–é‚£äº›urlçš„åˆ—è¡¨

parse1()->parse2()->...->trans()
ä¸€å¼€å§‹ä¼šåˆå§‹çš„urlï¼Œç„¶åæŠŠresponseä¼ ç»™parse1ï¼Œparse1è¿›è¡Œå¤„ç†åœ¨ä¼ ç»™parse2æœ€åç»™ä¸€ä¸ªå‡½æ•°(trans)å½¢æˆitemä¼ ç»™pipline
<h4>parseå‡½æ•°</h4>
å®šä¹‰parseå‡½æ•°ï¼Œé»˜è®¤æ¥å—ä¸€ä¸ªresponseå¯¹è±¡<br/>
responseå¯¹è±¡:<br/>

<div align= center><img src="./static/resobject.png"/></div>
è¿™ä¸ªå¯¹è±¡çš„åŸºæœ¬çš„urlï¼Œtextï¼Œstatusï¼Œencodingï¼Œheadersæ˜¯åŸºæœ¬å±æ€§ã€‚é‡ç‚¹è¯´ä¸€ä¸‹metaï¼Œselectorå’Œrequest
<br/>requestæ˜¯ä¸€ä¸ªRequestå¯¹è±¡ï¼Œæ˜¯ä¸€ä¸ªå®Œæ•´çš„è¯·æ±‚ä¿¡æ¯ï¼Œè§£æå»é‡æ¨¡å—ä¸­å°†è¦ä½¿ç”¨çš„å°±æ˜¯è¿™ä¸ªrequestä¸­çš„ä¿¡æ¯ã€‚<br/>
åœ¨*å¼‚æ­¥ä»£ç *ä¸­ä¸è¦ä½¿ç”¨åŒæ­¥ä»£ç ï¼Œä¼šå¯¼è‡´é˜»å¡æ€§èƒ½ä¸‹é™ä¸¥é‡ã€‚<br/>
è‡ªåŠ¨å¾€ä¸‹çˆ¬å–<br/>

ä»è¿™ä¸ªresponseä¸­æå–å‡ºè¿˜æœ‰éœ€è¦çˆ¬å–çš„urlæˆ‘ä»¬éœ€è¦ç»§ç»­è®¿é—®ï¼Œè¿™æ—¶å€™æˆ‘ä»¬è¦ç”¨Requestå‡½æ•°<br/>
```py
from scrapy import Request
def func(self, response):
  response.meta[k]#è¿™æ ·å°±å’Œå­—å…¸ä¸€æ ·å–å‡ºäº†v
  process...
  yield Request(url='localhost',meta={k:v},callback=self.func)
```
è¿™æ—¶å€™ä¼šyieldä¸€ä¸ªRequestå¯¹è±¡å‡ºå»ï¼Œè¯·æ±‚ä¸€ä¸‹ä¼ å…¥çš„urlï¼ŒæŠŠæœ¬æ¬¡processè§£æçš„ä¸œè¥¿ä½œä¸ºä¸€ä¸ª*å­—å…¸*ä¼ ç»™meta,è¿™æ ·å°±å®ç°äº†è§£æå†…å®¹çš„ä¼ é€’
:::tip
å¯ä»¥ç›´æ¥æŒ‡å‘self,è¿™æ ·è¿™ä¸ªè¯·æ±‚è¿˜ä¼šå›æ¥è°ƒç”¨è‡ªèº«ï¼Œå½¢æˆä¸€ä¸ªéšå¼é€’å½’ã€‚<br/>
æ³¨æ„é€’å½’çš„è¾¹ç•Œæ¡ä»¶å³urlçˆ¬å–åˆ°ä»€ä¹ˆæ—¶å€™è¦ä¼ å…¥transå‡½æ•°å­˜å…¥<br/>
å¦åˆ™å°±ä¼šä¸€ç›´æ‰§è¡Œä¸‹å»ç›´åˆ°æ‰€æœ‰çš„urléƒ½è®¿é—®è¿‡äº†
:::
æœ€å¸¸è§çš„æƒ…å†µå³è§£æä¸€ä¸ªé¡µé¢ï¼Œæ‹¿ä¸€äº›å†…å®¹åœ¨metaä¸­ä¼ é€’ç»™transå‡½æ•°æˆ–ä¸‹ä¸€ä¸ªparseï¼Œç„¶åå†è¿™ä¸ªé¡µé¢é‡Œçš„æŸäº›url(æ¯”å¦‚ä¸‹ä¸€é¡µç­‰)ç»§ç»­Requestè°ƒç”¨è‡ªèº«
<br/>Requestçš„å‚æ•°
- urlæ˜¯è¯·æ±‚çš„åœ°å€
- callback è¯·æ±‚å®Œä¹‹åçš„å›è°ƒï¼Œä¼ å…¥ä¸€ä¸ªresponse
- method='GET' è¯·æ±‚æ–¹æ³•Get/Post
- headers è¯·æ±‚å¤´
- body è¯·æ±‚body
- cookies ä½¿ç”¨å“ªä¸ªcookie
- meta metaç”¨æ¥ä¼ é€’å€¼
- encoding='utf-8' ç¼–ç 
- priority=0 åœ¨schedulerä¸­çš„ä¼˜å…ˆçº§
- dont_filter=False è¦ä¸è¦è¢«schedulerå»é‡
:::tip
headerï¼Œcookieï¼ŒUAå¯ä»¥åœ¨è¿™é‡ŒåŠ ä¹Ÿå¯ä»¥åœ¨middlewareé›†ä¸­åŠ åœ¨è¿™é‡ŒåŠ åªä¼šå¯¹è¿™ä¸ªRequestå‘èµ·çš„è¯·æ±‚æœ‰æ•ˆå…¶ä»–Requestä½¿ç”¨middlewareçš„
:::


<h4>å½¢æˆItemä¼ é€’ç»™pipline</h4>

trans()<br/>
å®šä¹‰ä¸€ä¸ªè§£æå‡½æ•°,è¿™ä¸ªå‡½æ•°æ¥å—ä¸€ä¸ªresponse,å®ƒä¼šè§£æresponseä¸­çš„å†…å®¹ï¼Œåˆ©ç”¨Xpathæˆ–è€…CssSelectoré€‰æ‹©è‡ªå·±æƒ³è¦çš„å†…å®¹
æœ€åå½¢æˆä¸€äº›å˜é‡ï¼Œå°†è¿™äº›å˜é‡é€šè¿‡item\['k']æˆ–Itemloaderèµ‹å€¼ç»™itemæœ€åyieldå‡ºå»
<h3>ä½¿ç”¨item</h3>

```py
#æ–‡ä»¶é¡¶éƒ¨å¼•å…¥
from engine.items import MyItem
...
item=MyItem()
item['x']=å˜é‡a
item['y']=å˜é‡b
yield item

```
è¿™ä¸ªitemä¼šä¼ åˆ°piplineä¸­å»

### CrawlSpider
å…¨ç«™çˆ¬è™«éœ€è¦æ³¨æ„çš„åœ°æ–¹æœ‰å»é‡ï¼Œurlå¤„ç†ï¼ŒRequestè½¬å‘<br/>
ä¸å†éœ€è¦è‡ªå·±å¯»æ‰¾urlï¼Œscrapyä¼šè‡ªå·±å¤„ç†é€šè¿‡*Rule*è¿›è¡Œç­›é€‰ï¼Œç¬¦åˆallow_dominså’Œruleçš„å°±ä¼šè‡ªåŠ¨æŠ“å–ï¼Œç„¶åè§£æè¯¥é¡µé¢æ‰€æœ‰åŒ…å«çš„url
,ç¬¦åˆæ¡ä»¶çš„ç»§ç»­é€’å½’Ruleä½¿ç”¨æ­£åˆ™è¿›è¡Œå®šä¹‰
æ–°å»ºä¸€ä¸ªä¹‹åæ˜¯è¿™æ ·çš„(ä¸ºäº†ä¾¿äºè®²è§£æˆ‘æ”¹è¿‡äº†)
<div align= center><img src="./static/crawl.png"/></div>
è¿™é‡Œruleæ˜¯ä¸€ä¸ªLinkExtractorå®šä¹‰çš„ç±»

- allow(æ­£åˆ™åŒ¹é…ç¬¦åˆçš„å°±è¯·æ±‚)
- callback(è¯·æ±‚ä¹‹åä¼ å…¥å“ªä¸ªå‡½æ•°,ä¸€èˆ¬æ˜¯ä¼ å…¥è¦è¿›è¡Œè§£æçš„å‡½æ•°)
- follow(ç¬¦åˆæœ¬è§„åˆ™çš„é¡µé¢æ˜¯å¦ç»§ç»­å¾€ä¸‹çˆ¬)
- uniqueï¼ˆbooleanï¼‰æ˜¯å¦å»é‡
- attrsï¼ˆlistï¼‰åœ¨æŸ¥æ‰¾è¦æå–çš„é“¾æ¥æ—¶åº”è¯¥è€ƒè™‘çš„å±æ€§æˆ–å±æ€§åˆ—è¡¨ï¼ˆä»…é€‚ç”¨äºå‚æ•°ä¸­æŒ‡å®šçš„é‚£äº›æ ‡ç­¾tags ï¼‰ã€‚é»˜è®¤ä¸º('href',)

### Xpathå’ŒCssé€‰æ‹©å™¨
<h3>è¯­æ³•çœ‹<a href='/çˆ¬è™«.html#xpath-cssselector'>è¿™é‡Œ</a></h3>
<BR/>

ä»ç½‘é¡µæºä»£ç ä¸­æå–è‡ªå·±æƒ³è¦çš„å­—æ®µå°±è¦ç”¨scrapyä¸­çš„æå–å™¨(scrapyçš„æå–å™¨é€Ÿåº¦å¿«ï¼Œå¼‚æ­¥æ“ä½œå‹å¥½)selector
<br/>
- selector=response.css('css selector')<br/>
- selector=response.xpath('xpath')<br/>

æ‹¿å‡ºæ¥çš„æ˜¯ä¸€selectorå¯¹è±¡æˆ–è€…æ˜¯SelectorList(åœ¨ç”±å¤šä¸ªèŠ‚ç‚¹æƒ…å†µä¸‹),è°ƒç”¨selector.extract[0],è·å–å†…å®¹
è¿™ä¸ªæ–¹æ³•ç­‰ä»·äºselector.extract_first()
è¿™é‡Œæ˜¯å†çˆ¬ç®€ä¹¦çš„æ—¶å€™ï¼Œæ‹¿åˆ°è¿™ä¸ªulçš„åˆ—è¡¨ä¹‹åxpathå¾—åˆ°çš„æ˜¯ä¸‹é¢æ‰€æœ‰çš„å…ƒç´ èŠ‚ç‚¹çš„SelectorListå¯¹è±¡ï¼ŒåŒ…å«å­èŠ‚ç‚¹çš„seector,å¯ä»¥å…ˆè°ƒç”¨
å±æ€§åƒè¿™æ ·<br/>

<div align= center><img style='height:440px' src="./static/selectorpro.png"/></div>
ä¸Šé¢çš„ä¸‹åˆ’çº¿å¼€å¤´çš„æ˜¯metaå±æ€§æœ€å¥½ä¸è¦è¿›è¡Œæ“ä½œï¼Œselectorçš„attr['pros']ç›´æ¥è·å–å¯¹è±¡çš„å±æ€§
<div align= center><img style='height:440px' src="./static/selector.png"/></div>
selectorå¯ä»¥ç»§ç»­è°ƒç”¨csså’Œxpathå¦‚ä¸Šå›¾

:::tip
urllib.parse import urljoin<br/>
pageurl=urljoin(base="https://www.jianshu.com",url=i.attrib["href"])<br/>
urljoinå¯ä»¥æŠŠurlå’ŒåŸŸåè¿æ¥èµ·æ¥ï¼Œä¿è¯ä¸ä¼šå‡ºé”™
:::


### ä½¿ç”¨Itemloader
<h4>ä½¿ç”¨itemloaderä¾¿äºç»´æŠ¤</h4>

from scrapy.loader import ItemLoader<br/>
åŒæ ·ï¼Œä½ éœ€è¦å®ä¾‹åŒ–ä¸€ä¸ªItem
item=ItemLoader(item=JianshuItem(),response=response)<br/>
æŠŠè‡ªå·±å¯¼å…¥çš„itemå’Œå‡½æ•°æ¥å—å¾—åˆ°çš„responseä¼ ç»™å®ƒ
æ¥ä¸‹æ¥ä½ å¯ä»¥æ·»åŠ å±æ€§
ä¸»è¦ç”¨
- item.add_xpath('FieldName','ä¸€ä¸ªxpath')
- item.add_css('FieldName','ä¸€ä¸ªcssselector')
- item.add_value('FieldName',éšä¾¿ä»€ä¹ˆå˜é‡)
æœ€åè®°å¾—return item.load_load()å‡ºå»
```py
def parse_item(self,response):
    item=ItemLoader(item=JianshuItem(),response=response)
    item.add_xpath('title','/html/body/div[1]/div[2]/div[1]/h1/text()')
    item.add_value('likes',re.findall('<script type="application/json" data-name="page-data">(.*?)</script>',response.body.decode('utf-8')))
    item.add_css('author','body > div.note > div.post > div.article > div.author > div > span > a::text')
    item.add_value('noteid',response.url)
    item.add_value('imageurl',response.meta['imageurl'])
    return item.load_item()


```
:::danger æ³¨æ„
item_loaderå’Œextracté»˜è®¤å§itemèµ‹å€¼éƒ½æ˜¯ä»¥listçš„å½¢å¼
:::
<h3>ç®€ä¹¦åˆ†æ</h3>

ç®€ä¹¦è¿™æ ·çš„ç½‘ç«™æ–‡ç« æ˜¯htmlé‡Œé¢ç›´æ¥å­˜åœ¨çš„ï¼Œè€Œä¸‹é¢çš„ä½œè€…ï¼Œè¯„è®ºï¼Œå–œæ¬¢æ‰“èµæ˜¯jsåŠ è½½å‡ºæ¥çš„
ç›´æ¥è·å–xpathå’Œcssselectoræ˜¯ä¸è¡Œçš„
<div align= center><img style='height:340px' src="./static/delete.png"/></div>
<h4>å»æ‰è¿™å‡ ä¸ªjsåä¼šè¿™æ ·</h4>

<div align= center><img style='height:340px' src="./static/effect.png"/></div>

æ•°æ®æ˜¯éšç€htmlä¸­çš„ä¸€ä¸ª\<script type="application/json" data-name="page-data">æ¥çš„

<div align= center><img style='height:340px' src="./static/datasource.png"/></div>

ç”¨æ­£åˆ™åŒ¹é…åˆ°è¿™ä¸ªå­—ç¬¦ä¸²ï¼Œåœ¨piplineä¸­æ‹¿å‡ºæˆ‘ä»¬æƒ³è¦çš„æ•°æ®å³å¯
## Item

### å®šä¹‰itemçš„Filed
Itemå®šä¹‰åœ¨items.pyé‡Œé¢ä¸€èˆ¬è¿™ä¸ªå­—æ®µæ˜¯ä½ æƒ³å¯¹åº”åˆ°dbé‡Œé¢çš„å­—æ®µ,ç”¨scrapy.Field()æ¥å®šä¹‰,åœ¨spiderä¸­èµ‹å€¼çš„æ—¶å€™å°±å¯ä»¥åƒç»™å­—å…¸èµ‹å€¼ä¸€æ ·ç›´æ¥èµ‹å€¼
åªæœ‰è¢«å®šä¹‰çš„å­—æ®µæ‰å¯ä»¥è¢«èµ‹å€¼ã€‚
å¯ä»¥åšä¸€äº›å¤„ç†å¤„ç†è¿‡åæ‰æ”¾åˆ°piplinesä¸­å»<br/>


<div align= center><img style='height:340px' src="./static/items.png"/></div>


### è¾“å…¥è¾“å‡ºå¤„ç†å™¨



åœ¨Fieldä¸­å¯ä»¥å®šä¹‰è¾“å…¥å¤„ç†å’Œè¾“å‡ºå¤„ç†å™¨(å°±æ˜¯ç”Ÿå‘½å‘¨æœŸé’©å­)ï¼ŒæŒ‡å®šä¸€ä¸ªå‡½æ•°ï¼Œ
å½“itemè¢«returnçš„æ—¶å€™å…ˆè°ƒç”¨è¾“å…¥å†è°ƒç”¨è¾“å‡ºå¤„ç†å¯¹å­—æ®µåŠ å·¥ç„¶åç»™pipline
æ˜¯åœ¨returnçš„æ—¶å€™æ‰ä¼šè¿›è¡Œ<br/>
é»˜è®¤çš„æœ‰å‡ ç§processorï¼Œå¯ä»¥å®šä¹‰ä¸€ä¸ªå‡½æ•°ç”¨MapComposeè‡ªå·±æ·»åŠ <br/>
SelectJmesï¼ŒIdentityï¼ŒTakeFirstï¼ŒComposeï¼ŒMapCompose
<br/>

input_processor=MapCompose(ä¸€ä¸ªåŒ¿åå‡½æ•°),<br/>
output_processor=TakeFirst(),<br/>
è¿™é‡Œçš„å€¼æ²¡æœ‰å˜ï¼Œç”¨ä¸€ä¸ªåŒ¿åå‡½æ•°åˆæ­¥å¤„ç†ä¸€ä¸‹æƒ³è¦ç›´æ¥å¤„ç†æ¸…æ¥šå¯ä»¥ç›´æ¥ç”¨pipline


```py

def callme(value):
    print("æˆ‘è¢«è°ƒç”¨å•¦")
    return value

class JianshuItem(scrapy.Item):

    title=scrapy.Field()
    likes=scrapy.Field(
        output_processor=MapCompose(callme))#æŠŠè¿™ä¸ªé‡Œé¢çš„ä¸œè¥¿å‡ºæ¥
    author=scrapy.Field()
    imageurl=scrapy.Field(
        input_processor=MapCompose(lambda x: "http:"+x.split("?")[0])
    )
    noteid=scrapy.Field()

```

### django scrapy item
:::danger ç•™å‘
å†™ç®€ä¹¦çš„æ—¶å€™è¯•è¯•
:::
## Pipline

é¦–å…ˆåœ¨settingä¸­å¼€å¯piplineå–æ¶ˆITEM_PIPELINESçš„æ³¨é‡Š
è¿™ä¸ªå­—å…¸ä¸€æ ·çš„ç»“æ„æ˜¯æµç»é¡ºåº,æ•°å­—è¶Šå°è¿™ä¸ªpiplineä¼šå…ˆæµç»,itemå°±åƒæµç»ç®¡é“ä¸€æ ·ä¸€ä¸ªä¸€ä¸ªæµè¿‡piplineä¼šå–ä¸‹æˆ–è€…ä¿®æ”¹itemçš„å­—æ®µ,å­˜å…¥sqlæˆ–whateverç„¶åreturnå‡ºå»
è®©ä»–æµç»ä¸‹ä¸€ä¸ªpiplineåƒè¿™æ ·
```py
ITEM_PIPELINES = {
   'aa.pipelines.likesPipeline': 1,
   'aa.pipelines.SqlPipeline': 2,
}

```


<h3>å®šä¹‰process_item</h3>

é»˜è®¤æ¥å—ä¸€ä¸ªitemå’Œä¸€ä¸ªspiderï¼Œitemå°±æ˜¯ä½ åœ¨itemä¸­å®šä¹‰çš„,åƒè°ƒç”¨å­—å…¸ä¸€æ ·è°ƒç”¨item
æŒ‰ç…§ä¸Šé¢è¯´çš„é¡ºåºæœ€ååˆ«å¿˜äº†returnå‡ºå»ç»™ä¸‹ä¸€ä¸ªpipelineç”¨

```py
class MyPipline(object):
    process_item(self, item, spider):
      ...
      return item
```
åƒå–å­—å…¸ä¸€æ ·ç›´æ¥å–ç„¶åèµ‹å€¼å°±å¯ä»¥äº†
å…ˆç»è¿‡äº†likespipelineï¼Œæ‹¿å‡ºè¿™ä¸ªå­—ç¬¦ä¸²æˆ‘ä»¬æƒ³è¦çš„éƒ¨åˆ†(åšå¤„ç†)ï¼Œç„¶åæµåˆ°äº†SqlPipelineè¢«æ‰“å°äº†å‡ºæ¥
<div align= center><img style='height:440px' src="./static/pipeline.png"/></div>

### å›¾ç‰‡Pipeline
å¾ˆå¤šæƒ…å†µä¸‹æ˜¯ä¸‰ç§æœ€å¸¸ç”¨çš„media,file,imageæœ‰å¯¹åº”å†™å¥½çš„piplineå†…ç½®å¼•å…¥
```py
#piplinesæ–‡ä»¶ä¸­å¼•å…¥
from scrapy.pipelines.images import ImagesPipeline

settingä¸­æŒ‡æ˜ä¸¤ä¸ªå˜é‡
IMAGES_URLS_FIELD = "imageurl"#å“ªä¸ªå­—æ®µæ˜¯imageçš„url
IMAGES_STORE = os.path.join(project_dir,"image")#å­˜åœ¨å“ªä¸ªè·¯å¾„ä¸‹
æ·»åŠ è¿›piplinesä¸­
ITEM_PIPELINES = {
'scrapy.pipelines.images.ImagesPipeline':1,#æ™®é€šçš„å›¾ç‰‡pipeline
                  }
```
:::warning
è¿™ä¸ªimageurlåº”è¯¥æ˜¯ä¸ªåˆ—è¡¨å½¢å¼,ä¸è¦å–å‡ºæ¥ä¼šæŠ¥é”™
:::
<h4>è‡ªå®šä¹‰å­˜æ”¾è·¯å¾„çš„imagepipline</h4>

<br>

é¦–å…ˆå¼•å…¥å¹¶ç»§æ‰¿ImagesPipeline
é‡å†™def item_completed(self, results, item, info)
<br>è‡ªå·±æ‰“æ–­ç‚¹çœ‹é‡Œé¢çš„å˜é‡,æ”¹å˜æƒ³è¦çš„å˜é‡å³å¯
```py
class JobboleImagePipline(ImagesPipeline):#è‡ªå®šä¹‰çš„ç®¡é“è¿›è¡Œè·¯å¾„å­˜æ”¾
    def item_completed(self, results, item, info):
        for status,p in results:
            try:
                item["imagepath"]=p["path"]
                item["imageurl"]=item["imageurl"][0]
            except TypeError:
                item["imageurl"]="None"
                item["imagepath"] = "None"
        return item
 ```

### æ’å…¥sqlæ•°æ®åº“
åœ¨è¿™ä¸ªå‡½æ•°é‡Œå–å‡ºå€¼ç„¶åå­˜è¿›å»ä¾‹å­
```py
class SqlPipeline(object):#æ„é€ sqlè¯­å¥å¹¶æ‰§è¡Œå³å¯
    def process_item(self,item,spider):
        '''item={'author': ['åŸåŒ—å¬é›ª'],
        'imageurl': ['http://upload-images.jianshu.io/upload_images/14074951-f700da6a9b168924.jpg'],
        'likes': ['33'],
        'noteid': ['https://www.jianshu.com/p/3e652aafb69b'],
        'title': ['5å¤©æµæµªåŠ å¾·æ»¡éƒ½ï¼Œç»™ä½ çœ‹ä¸€ä¸ªä¸ä¸€æ ·çš„çœŸå®å°¼æ³Šå°”']}'''
        a={ k:v[0] for k,v in item.items()}#æ„é€ å­—ç¬¦ä¸²
        str1 = ",".join([i for i in a.keys()])
        str2 = "','".join([i for i in a.values()])
        sql="INSERT INTO jianshu ({0}) VALUES ('{1}')".format(str1,str2)#æ„é€ sql
        conn=pymysql.connect(host='127.0.0.1',port=3306,user='root',db='jobol',password='ä¸å‘Šè¯‰ä½ ',charset='utf8mb4')
        cur=conn.cursor()
        try:
            cur.execute(sql)
            conn.commit()
        except Exception as e:
            conn.rollback()
            print(e)
        finally:
            cur.close()
            conn.close()
        return item

```

æ‰“å°å‡ºæ¥çš„sqlåº”è¯¥åƒè¿™æ ·


      INSERT INTO jianshu
      (author,imageurl,likes,noteid,title)
      VALUES
      ('åŸåŒ—å¬é›ª','http://upload-images.jianshu.io/upload_images/14074951-f700da6a9b168924.jpg','33','https://www.jianshu.com/p/3e652aafb69b','5å¤©æµæµªåŠ å¾·æ»¡éƒ½ï¼Œç»™ä½ çœ‹ä¸€ä¸ªä¸ä¸€æ ·çš„çœŸå®å°¼æ³Šå°”')


### å¼‚æ­¥æ’å…¥sql
é¦–å…ˆå®šä¹‰æ–°çš„piplineï¼Œç„¶åå¼€å§‹ç¼–å†™å¼‚æ­¥æ’å…¥çš„pipline
éœ€è¦pywin32,twisted<br/>
<h4>å†™æ³•å¾ˆå›ºå®šï¼Œè®°ä½æ€ä¹ˆå†™ç†è§£è¿™æ˜¯åšä»€ä¹ˆçš„</h4>

```py
import pymysql
from twisted.enterprise import adbapi
import pymysql.cursors
#å…ˆå¼•å…¥

class asyPipeline(object):
    def __init__(self,dbpool):#é»˜è®¤ä¼ å…¥ä¸€ä¸ªè¿æ¥æ± 
        self.dbpool=dbpool

    @classmethod#å£°æ˜ä¸€ä¸ªç±»æ–¹æ³•
    def from_settings(cls,settings):
    #æ³¨æ„æ˜¯settingsæœ‰Sè¿™ä¸ªç±»æ–¹æ³•å’Œå®ä¾‹æ²¡æœ‰å…³ç³»,æ¥å—ä¸€ä¸ªsettingä¹Ÿå°±æ˜¯æˆ‘ä»¬çš„é…ç½®æ–‡ä»¶
    #æŠŠå˜é‡æ‹¿è¿‡æ¥ï¼Œä¼ å…¥adbpoolé‡Œé¢å®šä¹‰ä¸€ä¸ªdbpool
        dbpool=adbapi.ConnectionPool('pymysql',
                                     host=settings['MYSQL_HOST'],
                                     db=settings['DB_NAME'],
                                     user=settings['MYSQL_USER'],
                                     password=settings['MYSQL_PASSWOED'],
                                     charset=settings['CHARSET'],
                                     cursorclass=pymysql.cursors.DictCursor,
                                     use_unicode=True)
    #è¿™é‡Œå¯ä»¥å†™æ­»ä¹Ÿå¯ä»¥ç”¨è¿™ä¸ªæ–¹æ³•è¯»å–settingé‡Œé¢çš„é…ç½®ä¿¡æ¯(å»ºè®®)
        return cls(dbpool=dbpool)

    def process_item(self,item,spider):

        query=self.dbpool.runInteraction(self.do_insert, item)
        #æ‰§è¡Œï¼Œç›¸å½“äºloopå¯åŠ¨ï¼Œå®šä¹‰ä¸€ä¸ªå˜é‡æ¥ä»£è¡¨
        query.addErrback(self.handler)
        #åˆ¶å®šé”™è¯¯å¤„ç†å‡½æ•°

    def do_insert(self,cursor,item):
        '''æ‰§è¡ŒæŸ¥è¯¢çš„å‡½æ•°'''
        a = {k: v[0] for k, v in item.items()}  # æ„é€ å­—ç¬¦ä¸²
        str1 = ",".join([i for i in a.keys()])
        str2 = "','".join([i for i in a.values()])
        sql = "INSERT INTO jianshu ({0}) VALUES ('{1}')".format(str1, str2)  # æ„é€ sql
        cursor.execute(sql)

        #ä¸èƒ½ç”¨tryå¼‚æ­¥æ²¡æœ‰try
    def handler(self,failure):#å®šä¹‰é”™è¯¯å¤„ç†
        print(failure)#æ‰“å°é”™è¯¯



```
:::danger æ³¨æ„
ä¸èƒ½ç”¨tryï¼Œå¼‚æ­¥é‡Œé¢æ²¡æœ‰try
:::

### æ’å…¥mongo
### ä¿å­˜ä¸ºjson
```py

class JsonPipeline(object):
    def process_item(self,item,spider):
        a = {k: v[0] for k, v in item.items()}#æ ¼å¼åŒ–ä¸€ä¸‹
        with open(r'jianshu.json','a',encoding='utf-8') as f:
            json.dump(a,f,ensure_ascii=False)
            f.write('\n')
```

<div align= center><img style='height:440px' src="./static/cunchu.png"/></div>



## Middleware

æ˜¯ä¸€ä¸ªé’©å­ç³»ç»Ÿï¼Œæ³¨å…¥åˆ°ç”Ÿå‘½å‘¨æœŸä¸­å¯æ‰©å±•æ€§å¥½
donwloader middlewaerç”¨å¾—æœ€å¤šï¼Œ
spidermiddlewareæ²¡æ€ä¹ˆç”¨
æ¯æ¬¡çˆ¬çš„æ—¶å€™ç»è¿‡ä¸€æ¬¡ä¸­é—´ä»¶,åƒpiplineä¸€æ ·æµè¿‡æœ€ååœ¨è¿›è¡Œè¯·æ±‚<br/>
ä¸»è¦ä½œç”¨æ›´æ¢ä»£ç†IPï¼Œæ›´æ¢Cookiesï¼Œæ›´æ¢User-Agentï¼Œè‡ªåŠ¨é‡è¯•
<h4>é‡å†™process_request</h4>(self,request,spider):

### éšæœºUser-Agent
å¯ä»¥åœ¨settingé‡Œé¢åˆ¶å®šUA<br/>
*USER_AGENT = 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36'*
æ‰€æœ‰çš„éƒ½ä¼šç”¨è¿™ä¸ªUA<br/>
```py
import fake_useragent
class RandomUAMiddleware(object):
    def process_request(self,request,spider):
        ua=fake_useragent.UserAgent()
        request.headers.setdefault('User-Agent',ua.random)
```
ç½‘ä¸Šå¾ˆå¤šç”¨UAlistçš„ï¼ŒåŸºæœ¬å°±æ˜¯importä¸€ä¸‹ï¼Œç„¶årandomå–ä¸€ä¸‹<br/>

<h4>fake_uaäº†è§£ä¸€ä¸‹ï¼Ÿ</h4>
gitæœæ‰€ç¬¬ä¸€ä¸ªå°±æ˜¯ï¼Œå¯ä»¥ç›´æ¥pipå®‰è£…<br/>

åŸºæœ¬ä½¿ç”¨
```py
from fake_useragent import UserAgent
ua = UserAgent()
ua.random#ç›´æ¥éšæœºè¿”å›ä¸€ä¸ªå¤´
ua.ie#è¿”å›ieçš„å¤´
```
:::tip å¯ç”¨çš„å¤´
ie,msie,opera,chrome,google,firefox,ff,safari
:::

### IPä»£ç†
åŒæ ·æ˜¯æ“ä½œreuestçš„metaä¿¡æ¯

```py
import random
#from settings import IPpool
class IPproxyMiddleware(object):
    def process_request(self,request):
        '''å¼•å…¥ä¸€ä¸ªipæ± æˆ–è¯·æ±‚ä¸€æ¬¡(è´­ä¹°çš„)æ¥å£,éšæœºé€‰æ‹©ä¸€ä¸ªipå°±å¯ä»¥'''
        request.meta["proxy"] = random.choice(["https://222.223.115.30:51618"])
```

### ä½¿ç”¨cookie

```py
class cookieMiddleware(object):
    def process_request(self,request,spider):
        request.cookies ='é€šè¿‡æ¥å£æˆ–ä»€ä¹ˆæ–¹å¼æ‹¿åˆ°ä½ çš„cookie'

    def process_response(self, request, response, spider):
        '''
        å¯¹æ­¤æ¬¡å“åº”è¿›è¡Œå¤„ç†
        å¦‚æœè¯·æ±‚ä¸æˆåŠŸæ€ä¹ˆå¤„ç†
        å¦‚æœcookieå¤±æ•ˆæ€ä¹ˆåŠ
        '''
        pass
```

:::tip TIP
æœ€å¥½è‡ªå·±æ­å»ºä»£ç†æ± å’Œcookieæ± ï¼Œç”¨çš„æ—¶å€™ç›´æ¥è°ƒç”¨æ¥å£
<br>åç»­æˆ‘åœ¨å‡ºè¿™ä¸ª
:::

:::warning
è¯·æ±‚å¤´æºå¸¦çš„Cookieå¿…é¡»æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œä¸èƒ½ç›´æ¥è®¾ç½®æˆå­—ç¬¦ä¸²
:::

### è‡ªåŠ¨é‡è¯•ä¸­é—´ä»¶
settingçš„dowloadermiddlwareä¸­<br/>
*scrapy.downloadermiddlewares.retry.RetryMiddleware:6*
<h4>ç›¸å…³è®¾ç½®</h4><br/>
- RETRY_ENABLED: æ˜¯å¦å¼€å¯retry(True)

- RETRY_TIMES: é‡è¯•æ¬¡æ•°(3)

- RETRY_HTTP_CODECS: é»˜è®¤[500,502,503,504,408]



### é›†æˆselenium
seleniumå¯ä»¥ç›´æ¥åœ¨ä¸­é—´ä»¶ç›´æ¥å†™
```py
class seleniumMiddleware(object):
    def __init__(self):
        self.broswer = webdriver.Chrome(executable_path=r'D:\57635\chromedriver.exe')
        super().__init__()
    def process_request(self,request,spider):
        if spider.name=='jianshu':
            self.broswer.get(request.url)
            time.sleep(2)
            print('seleniumè®¿é—®{}'.format(request.url))

            return HtmlResponse(url=self.broswer.current_url,
                                body=self.broswer.page_source,
                                encoding='utf-8',
                                request=request)

```
ä½†æ˜¯å½“ä¸€ä¸ªspiderç»“æŸçš„æ—¶å€™æµè§ˆå™¨ä¹Ÿä¸ä¼šé€€å‡ºï¼Œè¿™æ ·å¯¼è‡´æ¶ˆè€—èµ„æº
<h4>åœ¨spiderä¸­æ˜¯ç›´æ¥å†™</h4>
åœ¨spiderä¸­æ·»åŠ 

```py
from selenium import webdriver
from scrapy.xlib.pydispatch import dispatcher
from scrapy import signals

...
    def __init__(self):
        self.broswer = webdriver.Chrome(executable_path=r'D:\57635\chromedriver.exe')
        super().__init__()
        dispatcher.connect(self.spider_closed,signals.spider_closed)#ä¿¡å·é‡

    def spider_closed(self,spider):
        print("å®Œæˆ")
        self.broswer.quit()#é€€å‡º

...

middlware:
import time
from scrapy.http import HtmlResponse
class seleniumMiddleware(object):

    def process_request(self,request,spider):
        if spider.name=='jianshu':
            spider.broswer.get(request.url)
            time.sleep(2)
            print('seleniumè®¿é—®{}'.format(request.url))

            return HtmlResponse(url=spider.broswer.current_url,
                                body=spider.broswer.page_source,
                                encoding='utf-8',
                                request=request)

```
è§£é‡Šä¸€ä¸‹ï¼Œè¿™é‡Œå¼•å…¥çš„æ˜¯dispatchå’Œä¿¡å·signalï¼Œsignalæ˜¯scrapyå„ç§å¯¹è±¡ç”Ÿå‘½å‘¨æœŸä¿¡å·ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œå†™çš„æ˜¯å½“ä¸€ä¸ªçˆ¬è™«ç»“æŸçš„æ—¶å€™æ‰§è¡Œself.spider_closedå‡½æ•°ï¼Œå’Œä¿¡å·ç»‘ä¸Šï¼Œè¿™é‡Œå’Œdjangoé‡Œé¢çš„ä¿¡å·å¾ˆåƒ
åé¢å†è®²
è¿™æ ·åœ¨ä¸€ä¸ªspiderç»“æŸçš„æ—¶å€™å°±ä¼šé€€å‡ºæµè§ˆå™¨

## Schduler&Other

### Schdulerå»é‡æœºåˆ¶

Schduleræ˜¯ä¸€ä¸ªè°ƒåº¦å™¨ï¼Œè¿™é‡Œé¢æ˜¯ä¸€äº›é˜Ÿåˆ—å­˜æ”¾äº†æ‰€æœ‰å¾…çˆ¬çš„å·²ç»çˆ¬äº†çš„ç­‰ç­‰ï¼Œæ•´ä¸ªæ¡†æ¶çš„é˜Ÿåˆ—éƒ½åœ¨è¿™é‡Œé¢,å»é‡å’Œåˆ†å¸ƒå¼å¼€å‘æ’ä»¶å¼€å‘çš„æ—¶å€™ä¼šç”¨åˆ°
åˆ©ç”¨setå»é‡

:::danger ç•™å‘
çœ‹æºç ä¹‹åå†å†™
:::
### ç”Ÿå‘½å‘¨æœŸä¿¡å·é‡

## åˆ†å¸ƒå¼å¼€å‘ç•™å‘

<Valine></Valine>

